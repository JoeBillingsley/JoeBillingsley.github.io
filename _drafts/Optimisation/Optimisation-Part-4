Once we start considering populations of solutions we can also consider how we can share information between succesful solutions. To illustrate this lets look back at our pub crawl problem again. Consider these two solutions and their representations: 

Both solutions have managed to solve one half of the problem; by sharing information between them we could generate a better solution. Of course you can't expect to get something this convenient very often but similar situations are very common. The key idea here is that good solutions are constructed from good building blocks, components that work together for whatever reason and that with crossover we want to share these building blocks between solutions to try and construct better solutions.

We can use crossover to generate a new population similarily to how we used mutation before however now we need to select at least two solutions to exchange information from. Commonly we will choose two 'parents' from the current population to generate one or more 'child' solutions. To do so we can use any of the selection methods we discussed in the previous post to find two solutions. For example we could use tournament selection, allow two or more solutions to enter the tournament, keep the winner as one of the parents and repeat the process for the second parent.

[Figure of selection and crossover process / GA process]

Looking at our string from before we can see that there are building blocks we want to include but it is not clear where we want them to be placed. We could design an operator that could work on any string. For example in *uniform crossover* we simply iterate over both parents. For each variable we flip a coin to decide whether to decide which parent to take the variable from. We can create another child at the same time from the parts that were not selected.

[Animation/Figure of uniform crossover]

Whilst this crossover operator could work on any string we are not taking advantage of our particular problems characteristics. From our knowledge of pubs we know that they often end up on the same street. Hence good solutions probably have sections where nearby pubs are near to each other in the string. Therefore it makes sense to try and share contigous blocks of pubs. To do so we can use another very common and general purpose operator called N-Point Crossover. Here we make N random cuts along the string and exchange the variables in these regions to make one or two children.

[Animation/Figure of N-Point Crossover]
[Include in figure: Common settings are 1 or 2 cuts.]

So far we've ignored the key constraint of the pub crawl problem: we must visit every pub exactly once. With the N-Point and unifrom crossover operators we can easily break this constraint.

[Figure of constraint violation?]

With this in mind we can design a better operator for this particular problem. Position Based Crossover tries to preserve the position of cities over the two strings as much as possible whilst ensuring no variable is repeated twice. TODO: Position based crossover algorithm

[Pseudocode]

[Figure]

Position based crossover is a serious step up as we are now able to reliably generate child solutions but we can still do better. So far we have been thinking in terms of the position of cities in a string but that isn't the fundamental building block here. In fact we don't mind where in a string cities are, we are interested in the relationship between them - in particular the order in which each city appears relative to each other. This insight leads to Order Based Crossover and a collection of similar algorithms that aim to preserve as much about the order of cities in relation to each other as possible.

[Pseudocode]
[Figure]

However just because a problem may have some dependencies between components doesn't mean we can't share useful information between solutions. Looking at the pub crawl problem again we could see that we need to visit the pubs in the right order. Hence we could design a crossover operator that swaps the position of pubs in a string. There are a few common techniques for sharing information between two strings we could use:


Finally we may be able to find a better solution if we change our thinking entirely. TODO: The one where the representation is different so we can use conventional crossover operators.

Whether this particular case is a good idea or not is up for debate. The advantage of this representation is that it allows us to use standard generic crossover operators again. But this new representation does not necessarily encode information about the placement of pubs in relation to each other - something which we identified earlier as a key building block.

- 

Before we continue we should briefly mention the idea of **seperability**. This is the notion that some problems can be solved by finding the correct solution to each variable in turn. Consider the very simple problem OneMax for example:

\Sum_{i=0}^n X[i] where X \in {0,1}^n

In this problem, we want to turn a binary string into a string of 1's. This is clearly a separable problem as no variable has any dependency on any other. If we change a variable to 1, the fitness of the whole solution goes up. Happy days!

However with a problem like our pub crawl problem above, or the original travelling salesman problem or indeed most problems worth solving there are some interactions between the variables. In combinatorial problems these effects may be non-obvious such as the above the placement of objects/destinations relative to each other is important. In numerical optimisation problems it is usually a little clearer as you can see all the parts. For example we can introduce some degree of non-seperability into the OneMax problem if we wanted:


The degree of seperability is an important concern when designing or evaluating crossover operators. If a problem is seperable, any generic crossover operator such as uniform or k-point crossover could work. If the problem is partly-seperable, that is you are able to identify some groups of variables that must be considered, then you should take advantage of this as it may greatly simplify your problem. Finally if the problem seems strictly non-seperable such as TSP then you will want to design a clever crossover operator that uses what insight you can gleam from the problem else your solutions children will be doomed to suffer poor fitness and short lives.

-

TODO: Double check difference between genetic algorithm/evolutionary algorithm
TODO: Need to talk a little bit about how solutions move about the decision space for PSO introduction to make sense

The Genetic Algorithm
Finally then we can combine everything we have learnt so far into one algorithm. If you've ever read about metaheuristics before you'll have come across the Genetic Algorithm before, usually with some unhelpful story about how it was inspired by the process of evolution.

The genetic algorithm is very simple:

PSEUDOCODE

This is a pretty simple algorithm, I expect any one could have devised it with a good understanding of the material so far. In fact it is so simple I don't think there is anything more I can say about it. If you understand the parts then you should understand the system, at least intuitively.

What is worth pointing out though is the process. Selection, variation, evaluation, repeat. If you look closely you can find this process at the heart of any complex competitive system - whether thats the metaheuristics we've looked at so far such as hill climbing, tabu search, simulated annealing etc or real world systems: politics, fashion, the spread of ideas\cite{Darwin struck on this idea at about the same time as he figured out evolution. The idea is that knowledge, behaviour and ideas is shared amongst a culture and faces the same selective pressure as humans and animals. Richard Dawkins coined the term 'memes' for the idea and it stuck. Sometime later on internet memes started to spread...} and diseases and of course the evolution of species.

Over the remainder of this series we will be taking this idea seeded by GA and stretching it as far as we can, looking at some of the weird and wonderful algorithms people have come up with. Past this point you may find it harder to say what problems an algorithm will be suitable for or why but that is not the aim here. Instead each algorithm that is presented will have one or two key ideas that have since spread through the literature. Where appropriate I will try and include some more in depth discussion about the theory in additional bonus posts, starting with this one here about why maybe the genetic algorithm isn't quite as simple as I've let on.