If you are familiar with it this will sound similar to the issues when managing a cloud computing environment and in many ways it is very similar. In cloud computing the challenge is similar, people pay you to be able to provide a certain amount of computing resource whenever you need it. So for example if you need a server that has 16 processors in it, Amazon will happily rent you one from one of their datacentres. With cloud computing we don't generally have to worry about where in the datacentre we place the network functions because we are just selling resources. However when we're placing VNFs we are not usually selling resources but selling a certain quality of service. So instead of buying a server your buying a guarantee that the service you are providing will have low latency, high throughput and 99.99999% uptime. This moves the hard decision of choosing how much resources are needed from the consumer to the service provider. Now to make these decisions at a huge scale we need to move this intelligent decision making from humans to computers. And the first step to doing this, in my mind at least, is to model it and to really understand how the problem fits together.

There are two main schools of thought with regards to solving an optimisation problem. There are algorithms based in calculus and there are those based in probability. The calculus based approaches are your more classical, often pre-computer ways of doing things that work very well if you can map your problem to some equation and a set of constraints it needs to meet. The probability approaches are far more general, you can apply any metaheuristic to any problem but as we discussed before the more information you can include in the design of your algorithm the better the result will likely be.